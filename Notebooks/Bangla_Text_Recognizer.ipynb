{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Setup and Installations"
      ],
      "metadata": {
        "id": "2chvplegOWKO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lvTN3UKC53_r",
        "outputId": "1bf4dbc5-a968-4de1-c3a4-925e4b77ec14"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: google-cloud-vision in /usr/local/lib/python3.11/dist-packages (3.10.2)\n",
            "Requirement already satisfied: google-generativeai in /usr/local/lib/python3.11/dist-packages (0.8.5)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.12.0.88)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (11.3.0)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-vision) (2.25.1)\n",
            "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1 in /usr/local/lib/python3.11/dist-packages (from google-cloud-vision) (2.38.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-cloud-vision) (1.26.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2 in /usr/local/lib/python3.11/dist-packages (from google-cloud-vision) (5.29.5)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (0.6.15)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.178.0)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.11.7)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (4.14.1)\n",
            "Requirement already satisfied: numpy<2.3.0,>=2 in /usr/local/lib/python3.11/dist-packages (from opencv-python) (2.0.2)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-vision) (1.70.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.18.0 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-vision) (2.32.3)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-vision) (1.74.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-vision) (1.71.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-cloud-vision) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-cloud-vision) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-cloud-vision) (4.9.1)\n",
            "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (0.22.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (0.2.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (4.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai) (0.4.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google-generativeai) (3.2.3)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-cloud-vision) (0.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-vision) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-vision) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-vision) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-vision) (2025.8.3)\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "!pip install google-cloud-vision google-generativeai opencv-python pillow\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Libraries"
      ],
      "metadata": {
        "id": "ZvXCAa5HOaPV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from google.cloud import vision\n",
        "import google.generativeai as genai\n",
        "import re\n",
        "import time\n",
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "from difflib import SequenceMatcher\n",
        "from google.colab import files\n",
        "import glob"
      ],
      "metadata": {
        "id": "pcPXB8eU6aMO"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bengali Pipeline Class"
      ],
      "metadata": {
        "id": "CPMxfJ2yOd-N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BengaliAnalyzer:\n",
        "    def __init__(self):\n",
        "        self.vision_client = None\n",
        "        self.gemini_model = None\n",
        "        self.results_summary = []\n",
        "\n",
        "    def setup_apis(self, credentials_path, gemini_api_key):\n",
        "        \"\"\"Setup Google Vision and Gemini APIs\"\"\"\n",
        "        try:\n",
        "            os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = credentials_path\n",
        "            self.vision_client = vision.ImageAnnotatorClient()\n",
        "\n",
        "            genai.configure(api_key=gemini_api_key)\n",
        "            self.gemini_model = genai.GenerativeModel('gemini-1.5-flash')\n",
        "\n",
        "            print(\"APIs setup complete!\")\n",
        "            return True\n",
        "        except Exception as e:\n",
        "            print(f\"Setup failed: {e}\")\n",
        "            return False\n",
        "\n",
        "    def preprocess_image(self, image_path):\n",
        "        \"\"\"Image preprocessing\"\"\"\n",
        "        img = cv2.imread(image_path)\n",
        "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "        denoised = cv2.fastNlMeansDenoising(gray)\n",
        "        binary = cv2.adaptiveThreshold(denoised, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 11, 2)\n",
        "\n",
        "        _, encoded = cv2.imencode('.png', binary)\n",
        "        return encoded.tobytes()\n",
        "\n",
        "    def extract_text(self, image_bytes):\n",
        "        \"\"\"Extract text using Google Vision API\"\"\"\n",
        "        try:\n",
        "            image = vision.Image(content=image_bytes)\n",
        "            response = self.vision_client.text_detection(image=image)\n",
        "\n",
        "            if response.text_annotations:\n",
        "                return response.text_annotations[0].description.strip()\n",
        "            return \"\"\n",
        "        except Exception as e:\n",
        "            print(f\"Vision API error: {e}\")\n",
        "            return \"\"\n",
        "\n",
        "    def find_broken_words(self, text):\n",
        "        \"\"\"Find potentially broken words\"\"\"\n",
        "        broken_words = []\n",
        "        words = text.split()\n",
        "\n",
        "        for word in words:\n",
        "            has_bengali = bool(re.search(r'[\\u0980-\\u09FF]', word))\n",
        "            has_english = bool(re.search(r'[a-zA-Z]', word))\n",
        "\n",
        "            if (has_bengali and has_english) or (len(word) == 1 and has_bengali):\n",
        "                broken_words.append(word)\n",
        "\n",
        "        return broken_words\n",
        "\n",
        "    def correct_text(self, original_text, broken_words):\n",
        "        \"\"\"Correct text using Gemini\"\"\"\n",
        "        if not broken_words:\n",
        "            return original_text\n",
        "\n",
        "        try:\n",
        "            prompt = f\"\"\"Fix OCR errors in this Bengali text:\n",
        "\"{original_text}\"\n",
        "\n",
        "Broken words: {', '.join(broken_words)}\n",
        "\n",
        "Return only the corrected Bengali text:\"\"\"\n",
        "\n",
        "            response = self.gemini_model.generate_content(prompt)\n",
        "            corrected = response.text.strip()\n",
        "\n",
        "            if len(corrected) > 0 and len(corrected) < len(original_text) * 2:\n",
        "                return corrected\n",
        "            return original_text\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Gemini error: {e}\")\n",
        "            return original_text\n",
        "\n",
        "    def calculate_metrics(self, extracted_text, ground_truth):\n",
        "        \"\"\"Calculate accuracy metrics\"\"\"\n",
        "        if not ground_truth.strip():\n",
        "            return 0.0, 0.0, 0, 0, 0, 0\n",
        "\n",
        "        # Clean texts\n",
        "        extracted_clean = re.sub(r'\\s+', ' ', extracted_text.strip())\n",
        "        ground_truth_clean = re.sub(r'\\s+', ' ', ground_truth.strip())\n",
        "\n",
        "        # Character accuracy\n",
        "        char_accuracy = SequenceMatcher(None, extracted_clean, ground_truth_clean).ratio() * 100\n",
        "\n",
        "        # Word accuracy\n",
        "        extracted_words = extracted_clean.split()\n",
        "        ground_truth_words = ground_truth_clean.split()\n",
        "\n",
        "        correct_words = 0\n",
        "        for word in ground_truth_words:\n",
        "            if word in extracted_words:\n",
        "                correct_words += 1\n",
        "\n",
        "        word_accuracy = (correct_words / len(ground_truth_words)) * 100 if ground_truth_words else 0\n",
        "\n",
        "        # Character counts\n",
        "        correct_chars = int(len(ground_truth_clean) * char_accuracy / 100)\n",
        "        total_chars = len(ground_truth_clean)\n",
        "\n",
        "        return char_accuracy, word_accuracy, correct_words, len(ground_truth_words), correct_chars, total_chars\n",
        "\n"
      ],
      "metadata": {
        "id": "pouyV00r6hRu"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Configuration and Setup"
      ],
      "metadata": {
        "id": "H_ptm_PwOi3l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize analyzer\n",
        "analyzer = BengaliAnalyzer()\n",
        "\n",
        "# Upload credentials (run this once)\n",
        "print(\"Upload your Google Cloud credentials JSON file:\")\n",
        "uploaded_creds = files.upload()\n",
        "creds_filename = list(uploaded_creds.keys())[0]\n",
        "\n",
        "# Enter API key\n",
        "gemini_key = input(\"Enter your Gemini API key: \")\n",
        "\n",
        "# Setup APIs\n",
        "analyzer.setup_apis(creds_filename, gemini_key)\n",
        "\n",
        "# Define paths\n",
        "DRIVE_BASE_PATH = \"/content/drive/MyDrive\"  # Adjust if your drive structure is different\n",
        "DATASETS_PATH = f\"{DRIVE_BASE_PATH}/BT\"  # Folder containing D1, D2, D3, D4, D5\n",
        "OUTPUT_PATH = f\"{DRIVE_BASE_PATH}/BT/REC_OUTPUTS\"\n",
        "\n",
        "# Create output directory\n",
        "os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
        "\n",
        "print(f\"Looking for datasets in: {DATASETS_PATH}\")\n",
        "print(f\"Results will be saved to: {OUTPUT_PATH}\")"
      ],
      "metadata": {
        "id": "vzfo_zwx6neO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset Processing Function"
      ],
      "metadata": {
        "id": "HCIxs1VvOnyd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_dataset(dataset_name, dataset_path, analyzer):\n",
        "    \"\"\"Process a single dataset (D1, D2, etc.)\"\"\"\n",
        "\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Processing Dataset: {dataset_name}\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    # Define expected file names\n",
        "    expected_files = {\n",
        "        'clean': 'Clean_Handwriting.png',\n",
        "        'fast': 'Fast_Writing.png',\n",
        "        'ruled': 'Ruled_Paper.png',\n",
        "        'ground_truth': 'Typed_GroundTruth.txt'\n",
        "    }\n",
        "\n",
        "    # Check if all required files exist\n",
        "    missing_files = []\n",
        "    file_paths = {}\n",
        "\n",
        "    for file_type, filename in expected_files.items():\n",
        "        file_path = os.path.join(dataset_path, filename)\n",
        "        if os.path.exists(file_path):\n",
        "            file_paths[file_type] = file_path\n",
        "        else:\n",
        "            missing_files.append(filename)\n",
        "\n",
        "    if missing_files:\n",
        "        print(f\"Missing files in {dataset_name}: {', '.join(missing_files)}\")\n",
        "        return None\n",
        "\n",
        "    # Read ground truth\n",
        "    with open(file_paths['ground_truth'], 'r', encoding='utf-8') as f:\n",
        "        ground_truth = f.read().strip()\n",
        "\n",
        "    print(f\"Ground truth loaded: {len(ground_truth)} characters\")\n",
        "\n",
        "    # Process each image type\n",
        "    image_types = ['clean', 'fast', 'ruled']\n",
        "    dataset_results = []\n",
        "\n",
        "    for image_type in image_types:\n",
        "        print(f\"\\nProcessing {image_type} handwriting...\")\n",
        "\n",
        "        # Process image\n",
        "        image_bytes = analyzer.preprocess_image(file_paths[image_type])\n",
        "        extracted_text = analyzer.extract_text(image_bytes)\n",
        "\n",
        "        if extracted_text:\n",
        "            broken_words = analyzer.find_broken_words(extracted_text)\n",
        "            corrected_text = analyzer.correct_text(extracted_text, broken_words)\n",
        "            corrected_text = re.sub(r'\\s+', ' ', corrected_text).strip()\n",
        "        else:\n",
        "            corrected_text = \"\"\n",
        "\n",
        "        # Calculate metrics\n",
        "        char_acc, word_acc, correct_words, total_words, correct_chars, total_chars = \\\n",
        "            analyzer.calculate_metrics(corrected_text, ground_truth)\n",
        "\n",
        "        # Save individual result\n",
        "        output_filename = f\"{dataset_name}_{image_type}_output.txt\"\n",
        "        output_filepath = os.path.join(OUTPUT_PATH, output_filename)\n",
        "\n",
        "        with open(output_filepath, 'w', encoding='utf-8') as f:\n",
        "            f.write(corrected_text)\n",
        "\n",
        "        # Store results\n",
        "        result = {\n",
        "            'dataset': dataset_name,\n",
        "            'image_type': image_type,\n",
        "            'extracted_text': corrected_text,\n",
        "            'char_accuracy': char_acc,\n",
        "            'word_accuracy': word_acc,\n",
        "            'correct_words': correct_words,\n",
        "            'total_words': total_words,\n",
        "            'correct_chars': correct_chars,\n",
        "            'total_chars': total_chars,\n",
        "            'output_file': output_filename\n",
        "        }\n",
        "\n",
        "        dataset_results.append(result)\n",
        "\n",
        "        print(f\"{image_type}: {char_acc:.1f}% char, {word_acc:.1f}% word accuracy\")\n",
        "\n",
        "        time.sleep(1)  # Rate limiting\n",
        "\n",
        "    return dataset_results"
      ],
      "metadata": {
        "id": "ZbCNlat37OLG"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Process All Datasets"
      ],
      "metadata": {
        "id": "YgcITkKJOrDG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Process all datasets\n",
        "all_results = []\n",
        "\n",
        "for i in range(1, 6):  # D1 to D5\n",
        "    dataset_name = f\"D{i}\"\n",
        "    dataset_path = os.path.join(DATASETS_PATH, dataset_name)\n",
        "\n",
        "    if os.path.exists(dataset_path):\n",
        "        results = process_dataset(dataset_name, dataset_path, analyzer)\n",
        "        if results:\n",
        "            all_results.extend(results)\n",
        "    else:\n",
        "        print(f\"Dataset {dataset_name} not found at {dataset_path}\")\n",
        "\n",
        "print(f\"\\nProcessed {len(all_results)} image-dataset combinations\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "vmjXs6ch7aIN",
        "outputId": "fa103af0-7043-4967-f378-a1778d9bce4e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "Processing Dataset: D1\n",
            "============================================================\n",
            "Ground truth loaded: 1106 characters\n",
            "\n",
            "Processing clean handwriting...\n",
            "clean: 48.9% char, 51.0% word accuracy\n",
            "\n",
            "Processing fast handwriting...\n",
            "fast: 29.9% char, 47.1% word accuracy\n",
            "\n",
            "Processing ruled handwriting...\n",
            "ruled: 62.2% char, 47.1% word accuracy\n",
            "\n",
            "============================================================\n",
            "Processing Dataset: D2\n",
            "============================================================\n",
            "Ground truth loaded: 756 characters\n",
            "\n",
            "Processing clean handwriting...\n",
            "clean: 3.6% char, 0.9% word accuracy\n",
            "\n",
            "Processing fast handwriting...\n",
            "fast: 4.6% char, 0.9% word accuracy\n",
            "\n",
            "Processing ruled handwriting...\n",
            "ruled: 5.5% char, 0.9% word accuracy\n",
            "\n",
            "============================================================\n",
            "Processing Dataset: D3\n",
            "============================================================\n",
            "Ground truth loaded: 692 characters\n",
            "\n",
            "Processing clean handwriting...\n",
            "clean: 3.6% char, 2.7% word accuracy\n",
            "\n",
            "Processing fast handwriting...\n",
            "fast: 3.3% char, 2.7% word accuracy\n",
            "\n",
            "Processing ruled handwriting...\n",
            "ruled: 3.1% char, 2.7% word accuracy\n",
            "\n",
            "============================================================\n",
            "Processing Dataset: D4\n",
            "============================================================\n",
            "Ground truth loaded: 674 characters\n",
            "\n",
            "Processing clean handwriting...\n",
            "clean: 69.6% char, 56.5% word accuracy\n",
            "\n",
            "Processing fast handwriting...\n",
            "fast: 57.3% char, 56.5% word accuracy\n",
            "\n",
            "Processing ruled handwriting...\n",
            "ruled: 52.1% char, 59.3% word accuracy\n",
            "\n",
            "============================================================\n",
            "Processing Dataset: D5\n",
            "============================================================\n",
            "Ground truth loaded: 279 characters\n",
            "\n",
            "Processing clean handwriting...\n",
            "clean: 64.1% char, 82.2% word accuracy\n",
            "\n",
            "Processing fast handwriting...\n",
            "fast: 65.2% char, 51.1% word accuracy\n",
            "\n",
            "Processing ruled handwriting...\n",
            "ruled: 81.6% char, 82.2% word accuracy\n",
            "\n",
            "Processed 15 image-dataset combinations\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generate Comparison Table"
      ],
      "metadata": {
        "id": "2aUS7H0KOuOF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_comparison_table(results):\n",
        "    \"\"\"Create comprehensive comparison table\"\"\"\n",
        "\n",
        "    # Create DataFrame\n",
        "    df = pd.DataFrame(results)\n",
        "\n",
        "    if df.empty:\n",
        "        print(\"No results to display\")\n",
        "        return None\n",
        "\n",
        "    # Create detailed table\n",
        "    detailed_table = df[['dataset', 'image_type', 'char_accuracy', 'word_accuracy',\n",
        "                        'correct_words', 'total_words', 'correct_chars', 'total_chars']].copy()\n",
        "\n",
        "    # Round percentages\n",
        "    detailed_table['char_accuracy'] = detailed_table['char_accuracy'].round(1)\n",
        "    detailed_table['word_accuracy'] = detailed_table['word_accuracy'].round(1)\n",
        "\n",
        "    # Create summary by dataset\n",
        "    summary_by_dataset = df.groupby('dataset').agg({\n",
        "        'char_accuracy': 'mean',\n",
        "        'word_accuracy': 'mean',\n",
        "        'correct_words': 'sum',\n",
        "        'total_words': 'sum',\n",
        "        'correct_chars': 'sum',\n",
        "        'total_chars': 'sum'\n",
        "    }).round(1)\n",
        "\n",
        "    # Create summary by image type\n",
        "    summary_by_type = df.groupby('image_type').agg({\n",
        "        'char_accuracy': 'mean',\n",
        "        'word_accuracy': 'mean',\n",
        "        'correct_words': 'sum',\n",
        "        'total_words': 'sum',\n",
        "        'correct_chars': 'sum',\n",
        "        'total_chars': 'sum'\n",
        "    }).round(1)\n",
        "\n",
        "    # Overall summary\n",
        "    overall_summary = {\n",
        "        'total_datasets': df['dataset'].nunique(),\n",
        "        'total_images': len(df),\n",
        "        'avg_char_accuracy': df['char_accuracy'].mean(),\n",
        "        'avg_word_accuracy': df['word_accuracy'].mean(),\n",
        "        'total_correct_words': df['correct_words'].sum(),\n",
        "        'total_words': df['total_words'].sum(),\n",
        "        'total_correct_chars': df['correct_chars'].sum(),\n",
        "        'total_chars': df['total_chars'].sum()\n",
        "    }\n",
        "\n",
        "    return detailed_table, summary_by_dataset, summary_by_type, overall_summary\n",
        "\n",
        "# Generate tables\n",
        "detailed_table, summary_by_dataset, summary_by_type, overall_summary = create_comparison_table(all_results)\n",
        "\n"
      ],
      "metadata": {
        "id": "5Gy1GEWA8bI2"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Display Results"
      ],
      "metadata": {
        "id": "gpw_eUxrOxct"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"DETAILED RESULTS TABLE\")\n",
        "print(\"=\"*80)\n",
        "print(detailed_table.to_string(index=False))\n",
        "\n",
        "print(\"\\n\\nSUMMARY BY DATASET\")\n",
        "print(\"=\"*60)\n",
        "print(summary_by_dataset.to_string())\n",
        "\n",
        "print(\"\\n\\nSUMMARY BY HANDWRITING TYPE\")\n",
        "print(\"=\"*60)\n",
        "print(summary_by_type.to_string())\n",
        "\n",
        "print(\"\\n\\nOVERALL PERFORMANCE SUMMARY\")\n",
        "print(\"=\"*50)\n",
        "print(f\"Total Datasets Processed: {overall_summary['total_datasets']}\")\n",
        "print(f\"Total Images Processed: {overall_summary['total_images']}\")\n",
        "print(f\"Average Character Accuracy: {overall_summary['avg_char_accuracy']:.1f}%\")\n",
        "print(f\"Average Word Accuracy: {overall_summary['avg_word_accuracy']:.1f}%\")\n",
        "print(f\"Total Words Recognized: {overall_summary['total_correct_words']}/{overall_summary['total_words']} ({overall_summary['total_correct_words']/overall_summary['total_words']*100:.1f}%)\")\n",
        "print(f\"Total Characters Recognized: {overall_summary['total_correct_chars']}/{overall_summary['total_chars']} ({overall_summary['total_correct_chars']/overall_summary['total_chars']*100:.1f}%)\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EEoJdm-l8ejF",
        "outputId": "8cec32be-a0ea-4950-fc64-4f4c4e87fe1f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DETAILED RESULTS TABLE\n",
            "================================================================================\n",
            "dataset image_type  char_accuracy  word_accuracy  correct_words  total_words  correct_chars  total_chars\n",
            "     D1      clean           48.9           51.0             79          155            538         1101\n",
            "     D1       fast           29.9           47.1             73          155            329         1101\n",
            "     D1      ruled           62.2           47.1             73          155            684         1101\n",
            "     D2      clean            3.6            0.9              1          106             27          756\n",
            "     D2       fast            4.6            0.9              1          106             34          756\n",
            "     D2      ruled            5.5            0.9              1          106             41          756\n",
            "     D3      clean            3.6            2.7              3          111             24          688\n",
            "     D3       fast            3.3            2.7              3          111             22          688\n",
            "     D3      ruled            3.1            2.7              3          111             21          688\n",
            "     D4      clean           69.6           56.5             61          108            463          666\n",
            "     D4       fast           57.3           56.5             61          108            381          666\n",
            "     D4      ruled           52.1           59.3             64          108            346          666\n",
            "     D5      clean           64.1           82.2             37           45            178          279\n",
            "     D5       fast           65.2           51.1             23           45            181          279\n",
            "     D5      ruled           81.6           82.2             37           45            227          279\n",
            "\n",
            "\n",
            "SUMMARY BY DATASET\n",
            "============================================================\n",
            "         char_accuracy  word_accuracy  correct_words  total_words  correct_chars  total_chars\n",
            "dataset                                                                                      \n",
            "D1                47.0           48.4            225          465           1551         3303\n",
            "D2                 4.6            0.9              3          318            102         2268\n",
            "D3                 3.3            2.7              9          333             67         2064\n",
            "D4                59.6           57.4            186          324           1190         1998\n",
            "D5                70.3           71.9             97          135            586          837\n",
            "\n",
            "\n",
            "SUMMARY BY HANDWRITING TYPE\n",
            "============================================================\n",
            "            char_accuracy  word_accuracy  correct_words  total_words  correct_chars  total_chars\n",
            "image_type                                                                                      \n",
            "clean                38.0           38.7            181          525           1230         3490\n",
            "fast                 32.1           31.7            161          525            947         3490\n",
            "ruled                40.9           38.4            178          525           1319         3490\n",
            "\n",
            "\n",
            "OVERALL PERFORMANCE SUMMARY\n",
            "==================================================\n",
            "Total Datasets Processed: 5\n",
            "Total Images Processed: 15\n",
            "Average Character Accuracy: 37.0%\n",
            "Average Word Accuracy: 36.3%\n",
            "Total Words Recognized: 520/1575 (33.0%)\n",
            "Total Characters Recognized: 3496/10470 (33.4%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save Results to Drive"
      ],
      "metadata": {
        "id": "FhY59ERLO0UF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save detailed results to CSV\n",
        "detailed_table.to_csv(f\"{OUTPUT_PATH}/detailed_results.csv\", index=False)\n",
        "\n",
        "# Save summary tables\n",
        "summary_by_dataset.to_csv(f\"{OUTPUT_PATH}/summary_by_dataset.csv\")\n",
        "summary_by_type.to_csv(f\"{OUTPUT_PATH}/summary_by_type.csv\")\n",
        "\n",
        "# Save comprehensive report\n",
        "report_path = f\"{OUTPUT_PATH}/comprehensive_report.txt\"\n",
        "with open(report_path, 'w', encoding='utf-8') as f:\n",
        "    f.write(\"BENGALI HANDWRITING RECOGNITION - COMPREHENSIVE ANALYSIS\\n\")\n",
        "    f.write(\"=\"*70 + \"\\n\\n\")\n",
        "\n",
        "    f.write(\"DETAILED RESULTS:\\n\")\n",
        "    f.write(\"-\"*40 + \"\\n\")\n",
        "    f.write(detailed_table.to_string(index=False))\n",
        "    f.write(\"\\n\\n\")\n",
        "\n",
        "    f.write(\"SUMMARY BY DATASET:\\n\")\n",
        "    f.write(\"-\"*40 + \"\\n\")\n",
        "    f.write(summary_by_dataset.to_string())\n",
        "    f.write(\"\\n\\n\")\n",
        "\n",
        "    f.write(\"SUMMARY BY HANDWRITING TYPE:\\n\")\n",
        "    f.write(\"-\"*40 + \"\\n\")\n",
        "    f.write(summary_by_type.to_string())\n",
        "    f.write(\"\\n\\n\")\n",
        "\n",
        "    f.write(\"OVERALL PERFORMANCE:\\n\")\n",
        "    f.write(\"-\"*40 + \"\\n\")\n",
        "    f.write(f\"Total Datasets: {overall_summary['total_datasets']}\\n\")\n",
        "    f.write(f\"Total Images: {overall_summary['total_images']}\\n\")\n",
        "    f.write(f\"Avg Character Accuracy: {overall_summary['avg_char_accuracy']:.1f}%\\n\")\n",
        "    f.write(f\"Avg Word Accuracy: {overall_summary['avg_word_accuracy']:.1f}%\\n\")\n",
        "    f.write(f\"Total Words: {overall_summary['total_correct_words']}/{overall_summary['total_words']} ({overall_summary['total_correct_words']/overall_summary['total_words']*100:.1f}%)\\n\")\n",
        "    f.write(f\"Total Characters: {overall_summary['total_correct_chars']}/{overall_summary['total_chars']} ({overall_summary['total_correct_chars']/overall_summary['total_chars']*100:.1f}%)\\n\")\n",
        "\n",
        "print(f\"\\nResults saved to Google Drive:\")\n",
        "print(f\"   Individual outputs: {OUTPUT_PATH}/\")\n",
        "print(f\"   Detailed results: {OUTPUT_PATH}/detailed_results.csv\")\n",
        "print(f\"   Dataset summary: {OUTPUT_PATH}/summary_by_dataset.csv\")\n",
        "print(f\"   Type summary: {OUTPUT_PATH}/summary_by_type.csv\")\n",
        "print(f\"   Full report: {OUTPUT_PATH}/comprehensive_report.txt\")\n",
        "\n",
        "print(\"\\nMulti-dataset analysis completed successfully!\")"
      ],
      "metadata": {
        "id": "lz1rYa4q8lD9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}