{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Import Libraries"
      ],
      "metadata": {
        "id": "yfJ7IsF7XlDF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import unicodedata\n",
        "from collections import Counter, defaultdict\n",
        "import pandas as pd\n",
        "from typing import List, Dict, Tuple\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "import urllib.request"
      ],
      "metadata": {
        "id": "vRHqLLTyVdte"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Configure Bangla Fonts"
      ],
      "metadata": {
        "id": "-4pQkBRPX2cM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def setup_bangla_fonts():\n",
        "    \"\"\"Download and install Bangla fonts for matplotlib\"\"\"\n",
        "    font_dir = '/usr/share/fonts/truetype/bangla/'\n",
        "    os.makedirs(font_dir, exist_ok=True)\n",
        "\n",
        "    # Download Noto Sans Bengali font\n",
        "    font_url = 'https://github.com/googlefonts/noto-fonts/raw/main/hinted/ttf/NotoSansBengali/NotoSansBengali-Regular.ttf'\n",
        "    font_path = f'{font_dir}NotoSansBengali-Regular.ttf'\n",
        "\n",
        "    if not os.path.exists(font_path):\n",
        "        print(\"Downloading Bangla font...\")\n",
        "        urllib.request.urlretrieve(font_url, font_path)\n",
        "        print(\"Font downloaded successfully!\")\n",
        "\n",
        "        # Clear matplotlib font cache (updated method)\n",
        "        import matplotlib.font_manager as fm\n",
        "        import matplotlib.pyplot as plt\n",
        "\n",
        "        # Delete font cache and rebuild\n",
        "        try:\n",
        "            import shutil\n",
        "            cache_dir = fm.get_cachedir()\n",
        "            if os.path.exists(cache_dir):\n",
        "                shutil.rmtree(cache_dir)\n",
        "            fm.FontManager.__init__(fm.fontManager)\n",
        "            print(\"Font cache rebuilt successfully!\")\n",
        "        except Exception as e:\n",
        "            print(f\"  Font cache rebuild failed: {e}\")\n",
        "            print(\"You may need to restart the runtime for fonts to work properly\")\n",
        "\n",
        "    return 'Noto Sans Bengali'\n",
        "\n",
        "# Setup fonts\n",
        "font_name = setup_bangla_fonts()\n",
        "\n",
        "# Configure matplotlib with fallback fonts\n",
        "plt.rcParams['font.family'] = [font_name, 'DejaVu Sans', 'Liberation Sans', 'sans-serif']\n",
        "plt.rcParams['axes.unicode_minus'] = False  # Handle minus signs properly\n",
        "\n",
        "print(f\"Font configured: {font_name}\")\n",
        "print(\"If Bangla text doesn't display properly, restart runtime and run again\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7-DTXRC5X5fB",
        "outputId": "6c1bdc62-9cfe-4c7b-b98c-f686e6bd0b82"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Font configured: Noto Sans Bengali\n",
            "If Bangla text doesn't display properly, restart runtime and run again\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenizer Class"
      ],
      "metadata": {
        "id": "olUKAZGJYBzG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BanglaTokenizer:\n",
        "    \"\"\"\n",
        "    Bangla Text Tokenizer using Regular Expressions\n",
        "    Implements word-level and character-level tokenization for Bangla text\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        # Word-level tokenization patterns\n",
        "        self.word_patterns = {\n",
        "            'WORD': r'[\\u0980-\\u09FF]+',  # Bangla Unicode block\n",
        "            'NUMBER': r'[\\u09E6-\\u09EF]+',  # Bangla digits\n",
        "            'PUNCTUATION': r'[।,?!;:]|\\u0964|\\u0965',  # Bangla and borrowed punctuation\n",
        "            'WHITESPACE': r'\\s+',  # Standard whitespace\n",
        "            'LATIN_WORD': r'[a-zA-Z]+',  # Latin characters (for mixed text)\n",
        "            'ARABIC_NUM': r'[0-9]+',  # Arabic numerals\n",
        "            'OTHER': r'[^\\s]'  # Any other character\n",
        "        }\n",
        "\n",
        "        # Character-level tokenization patterns (hierarchical precedence)\n",
        "        self.char_patterns = {\n",
        "            'COMPOUND': r'[\\u0995-\\u09B9](?:\\u09CD[\\u0995-\\u09B9])+',  # Compound consonants\n",
        "            'CONSONANT_VOWEL': r'[\\u0995-\\u09B9][\\u09BE-\\u09CC]',  # Consonant + vowel diacritic\n",
        "            'CONSONANT': r'[\\u0995-\\u09B9]',  # Individual consonants\n",
        "            'INDEPENDENT_VOWEL': r'[\\u0985-\\u0994]',  # Independent vowels\n",
        "            'DEPENDENT_VOWEL': r'[\\u09BE-\\u09CC]',  # Dependent vowel signs\n",
        "            'MODIFIER': r'[\\u09BC\\u09CD\\u09D7]',  # Modifiers and diacritics\n",
        "            'BANGLA_DIGIT': r'[\\u09E6-\\u09EF]',  # Bangla digits\n",
        "            'PUNCTUATION': r'[।,?!;:]|\\u0964|\\u0965',  # Punctuation\n",
        "            'WHITESPACE': r'\\s',  # Individual whitespace\n",
        "            'LATIN_CHAR': r'[a-zA-Z]',  # Latin characters\n",
        "            'ARABIC_DIGIT': r'[0-9]',  # Arabic numerals\n",
        "            'OTHER': r'.'  # Any other character\n",
        "        }\n",
        "\n",
        "        # Compile patterns for efficiency\n",
        "        self.word_regex = self._compile_patterns(self.word_patterns)\n",
        "        self.char_regex = self._compile_patterns(self.char_patterns)\n",
        "\n",
        "    def _compile_patterns(self, patterns: Dict[str, str]) -> List[Tuple[str, re.Pattern]]:\n",
        "        \"\"\"Compile regex patterns with their labels\"\"\"\n",
        "        return [(label, re.compile(pattern)) for label, pattern in patterns.items()]\n",
        "\n",
        "    def normalize_text(self, text: str) -> str:\n",
        "        \"\"\"Apply Unicode NFC normalization\"\"\"\n",
        "        return unicodedata.normalize('NFC', text)\n",
        "\n",
        "    def word_tokenize(self, text: str) -> List[Dict[str, str]]:\n",
        "        \"\"\"\n",
        "        Perform word-level tokenization\n",
        "        Returns list of dictionaries with token and type\n",
        "        \"\"\"\n",
        "        normalized_text = self.normalize_text(text)\n",
        "        tokens = []\n",
        "        position = 0\n",
        "\n",
        "        while position < len(normalized_text):\n",
        "            matched = False\n",
        "\n",
        "            # Try each pattern in order\n",
        "            for label, pattern in self.word_regex:\n",
        "                match = pattern.match(normalized_text, position)\n",
        "                if match:\n",
        "                    token_text = match.group()\n",
        "                    if label != 'WHITESPACE' or token_text.strip():  # Include whitespace but filter empty\n",
        "                        tokens.append({\n",
        "                            'token': token_text,\n",
        "                            'type': label,\n",
        "                            'position': position,\n",
        "                            'length': len(token_text)\n",
        "                        })\n",
        "                    position = match.end()\n",
        "                    matched = True\n",
        "                    break\n",
        "\n",
        "            if not matched:\n",
        "                # Handle unmatched characters\n",
        "                tokens.append({\n",
        "                    'token': normalized_text[position],\n",
        "                    'type': 'UNKNOWN',\n",
        "                    'position': position,\n",
        "                    'length': 1\n",
        "                })\n",
        "                position += 1\n",
        "\n",
        "        return tokens\n",
        "\n",
        "    def character_tokenize(self, text: str) -> List[Dict[str, str]]:\n",
        "        \"\"\"\n",
        "        Perform character-level tokenization with compound character handling\n",
        "        Returns list of dictionaries with token and type\n",
        "        \"\"\"\n",
        "        normalized_text = self.normalize_text(text)\n",
        "        tokens = []\n",
        "        position = 0\n",
        "\n",
        "        while position < len(normalized_text):\n",
        "            matched = False\n",
        "\n",
        "            # Try patterns in precedence order (compound first)\n",
        "            for label, pattern in self.char_regex:\n",
        "                match = pattern.match(normalized_text, position)\n",
        "                if match:\n",
        "                    token_text = match.group()\n",
        "                    tokens.append({\n",
        "                        'token': token_text,\n",
        "                        'type': label,\n",
        "                        'position': position,\n",
        "                        'length': len(token_text)\n",
        "                    })\n",
        "                    position = match.end()\n",
        "                    matched = True\n",
        "                    break\n",
        "\n",
        "            if not matched:\n",
        "                # Fallback for unmatched characters\n",
        "                tokens.append({\n",
        "                    'token': normalized_text[position],\n",
        "                    'type': 'UNKNOWN',\n",
        "                    'position': position,\n",
        "                    'length': 1\n",
        "                })\n",
        "                position += 1\n",
        "\n",
        "        return tokens\n",
        "\n",
        "    def analyze_tokens(self, tokens: List[Dict[str, str]]) -> Dict:\n",
        "        \"\"\"Analyze tokenization results\"\"\"\n",
        "        token_texts = [t['token'] for t in tokens]\n",
        "        token_types = [t['type'] for t in tokens]\n",
        "\n",
        "        # Count statistics\n",
        "        total_tokens = len(tokens)\n",
        "        unique_tokens = len(set(token_texts))\n",
        "        token_freq = Counter(token_texts)\n",
        "        type_freq = Counter(token_types)\n",
        "\n",
        "        # Most common tokens\n",
        "        most_common_tokens = token_freq.most_common(10)\n",
        "\n",
        "        return {\n",
        "            'total_tokens': total_tokens,\n",
        "            'unique_tokens': unique_tokens,\n",
        "            'token_frequency': token_freq,\n",
        "            'type_frequency': type_freq,\n",
        "            'most_common_tokens': most_common_tokens,\n",
        "            'tokens': tokens\n",
        "        }"
      ],
      "metadata": {
        "id": "7tC4TbHEYDlS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Text File"
      ],
      "metadata": {
        "id": "urRGKB7jYKFJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_text_file(file_path: str) -> str:\n",
        "    \"\"\"Load text from file with proper encoding\"\"\"\n",
        "    if not os.path.exists(file_path):\n",
        "        raise FileNotFoundError(f\"Text file not found at: {file_path}\\n\"\n",
        "                              f\"Please upload a .txt file with Bangla text to: {file_path}\")\n",
        "\n",
        "    # Try different encodings\n",
        "    encodings = ['utf-8', 'utf-16', 'utf-8-sig', 'cp1252']\n",
        "\n",
        "    for encoding in encodings:\n",
        "        try:\n",
        "            with open(file_path, 'r', encoding=encoding) as f:\n",
        "                content = f.read().strip()\n",
        "                if content:  # Check if file is not empty\n",
        "                    print(f\"File loaded successfully with {encoding} encoding\")\n",
        "                    return content\n",
        "                else:\n",
        "                    raise ValueError(\"File is empty\")\n",
        "        except (UnicodeDecodeError, UnicodeError):\n",
        "            continue\n",
        "        except Exception as e:\n",
        "            raise Exception(f\"Error reading file: {str(e)}\")\n",
        "\n",
        "    raise UnicodeDecodeError(\"Could not decode the file with any supported encoding\")\n",
        "\n",
        "def get_sample_text() -> str:\n",
        "    raise NotImplementedError(\"Sample text is not provided. Please upload a .txt file with Bangla text.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "P7doEtE_YM7q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Results Generation"
      ],
      "metadata": {
        "id": "06JrwLDTYkDm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def print_detailed_results(word_analysis: Dict, char_analysis: Dict, sample_size: int = 20):\n",
        "    \"\"\"Print detailed tokenization results\"\"\"\n",
        "    print(\"BANGLA TEXT TOKENIZATION RESULTS\")\n",
        "    print(\" \")\n",
        "\n",
        "    # Summary Statistics\n",
        "    print(\"\\nSUMMARY STATISTICS\")\n",
        "    print(\" \")\n",
        "    print(f\"Word-level Tokenization:\")\n",
        "    print(f\"  • Total tokens: {word_analysis['total_tokens']:,}\")\n",
        "    print(f\"  • Unique tokens: {word_analysis['unique_tokens']:,}\")\n",
        "    print(f\"  • Type-token ratio: {word_analysis['unique_tokens']/word_analysis['total_tokens']:.3f}\")\n",
        "\n",
        "    print(f\"\\nCharacter-level Tokenization:\")\n",
        "    print(f\"  • Total tokens: {char_analysis['total_tokens']:,}\")\n",
        "    print(f\"  • Unique tokens: {char_analysis['unique_tokens']:,}\")\n",
        "    print(f\"  • Type-token ratio: {char_analysis['unique_tokens']/char_analysis['total_tokens']:.3f}\")\n",
        "\n",
        "    # Token Type Distribution\n",
        "    print(\"\\nTOKEN TYPE DISTRIBUTION\")\n",
        "    print(\" \")\n",
        "\n",
        "    print(\"Word-level:\")\n",
        "    for token_type, count in word_analysis['type_frequency'].items():\n",
        "        percentage = (count / word_analysis['total_tokens']) * 100\n",
        "        print(f\"  • {token_type:<15}: {count:>4} ({percentage:>5.1f}%)\")\n",
        "\n",
        "    print(\"\\nCharacter-level:\")\n",
        "    for token_type, count in char_analysis['type_frequency'].items():\n",
        "        percentage = (count / char_analysis['total_tokens']) * 100\n",
        "        print(f\"  • {token_type:<20}: {count:>4} ({percentage:>5.1f}%)\")\n",
        "\n",
        "    # Most Common Tokens\n",
        "    print(\"\\nMOST FREQUENT TOKENS\")\n",
        "    print(\" \")\n",
        "\n",
        "    print(\"Word-level (Top 10):\")\n",
        "    for i, (token, count) in enumerate(word_analysis['most_common_tokens'][:10], 1):\n",
        "        print(f\"  {i:>2}. '{token}' → {count} times\")\n",
        "\n",
        "    print(\"\\nCharacter-level (Top 15):\")\n",
        "    for i, (token, count) in enumerate(char_analysis['most_common_tokens'][:15], 1):\n",
        "        print(f\"  {i:>2}. '{token}' → {count} times\")\n",
        "\n",
        "    # Sample Tokens\n",
        "    print(f\"\\nSAMPLE TOKENS (First {sample_size})\")\n",
        "    print(\" \")\n",
        "\n",
        "    print(\"Word-level tokens:\")\n",
        "    word_tokens = [t for t in word_analysis['tokens'][:sample_size] if t['type'] != 'WHITESPACE'][:sample_size]\n",
        "    for i, token in enumerate(word_tokens, 1):\n",
        "        print(f\"  {i:>2}. [{token['type']:<12}] '{token['token']}'\")\n",
        "\n",
        "    print(f\"\\nCharacter-level tokens:\")\n",
        "    char_tokens = [t for t in char_analysis['tokens'][:sample_size*2] if t['type'] != 'WHITESPACE'][:sample_size]\n",
        "    for i, token in enumerate(char_tokens, 1):\n",
        "        print(f\"  {i:>2}. [{token['type']:<18}] '{token['token']}'\")\n",
        "\n"
      ],
      "metadata": {
        "id": "22r-xJGnYmTr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main Function"
      ],
      "metadata": {
        "id": "C8b7FoLLYp-_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    \"\"\"Main pipeline execution\"\"\"\n",
        "    print(\"Bangla Text Tokenization Pipeline Starting...\")\n",
        "    print(\"REQUIREMENTS:\")\n",
        "    print(\"   • Upload a .txt file containing Bangla text\")\n",
        "    print(\"   • Update the 'text_file_path' variable with your file path\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "    # Initialize tokenizer\n",
        "    tokenizer = BanglaTokenizer()\n",
        "\n",
        "    # Load text (USER MUST MODIFY THIS PATH)\n",
        "    text_file_path = \"/content/Typed_GroundTruth.txt\"  #\n",
        "\n",
        "    print(f\"Looking for text file at: {text_file_path}\")\n",
        "\n",
        "    try:\n",
        "        text = load_text_file(text_file_path)\n",
        "        print(f\"Input text length: {len(text)} characters\")\n",
        "        print(f\"Input text preview: {text[:100]}...\")\n",
        "\n",
        "        # Perform tokenization\n",
        "        print(\"\\nPerforming word-level tokenization...\")\n",
        "        word_tokens = tokenizer.word_tokenize(text)\n",
        "        word_analysis = tokenizer.analyze_tokens(word_tokens)\n",
        "\n",
        "        print(\"Performing character-level tokenization...\")\n",
        "        char_tokens = tokenizer.character_tokenize(text)\n",
        "        char_analysis = tokenizer.analyze_tokens(char_tokens)\n",
        "\n",
        "        # Display results\n",
        "        print_detailed_results(word_analysis, char_analysis)\n",
        "\n",
        "        # Export results to CSV (optional)\n",
        "        print(\"\\nExporting results...\")\n",
        "\n",
        "        # Word tokens DataFrame\n",
        "        word_df = pd.DataFrame(word_analysis['tokens'])\n",
        "        word_df.to_csv('/content/word_tokens.csv', index=False, encoding='utf-8')\n",
        "\n",
        "        # Character tokens DataFrame\n",
        "        char_df = pd.DataFrame(char_analysis['tokens'])\n",
        "        char_df.to_csv('/content/char_tokens.csv', index=False, encoding='utf-8')\n",
        "\n",
        "        print(\"Results exported to CSV files:\")\n",
        "        print(\"   • word_tokens.csv\")\n",
        "        print(\"   • char_tokens.csv\")\n",
        "\n",
        "        return word_analysis, char_analysis\n",
        "\n",
        "    except FileNotFoundError as e:\n",
        "        print(f\"ERROR: {e}\")\n",
        "        print(\"\\nTO FIX THIS:\")\n",
        "        print(\"1. Upload your Bangla text file to Colab\")\n",
        "        print(\"2. Update the 'text_file_path' variable in main() function\")\n",
        "        print(\"3. Make sure the file contains Bangla text in UTF-8 encoding\")\n",
        "        return None, None\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR: {e}\")\n",
        "        return None, None\n",
        "\n"
      ],
      "metadata": {
        "id": "6ocftHCYYrq2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Execute the pipeline"
      ],
      "metadata": {
        "id": "MIJnwa9KYtq0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Execute the pipeline\n",
        "if __name__ == \"__main__\":\n",
        "    # Check if results were successful before proceeding\n",
        "    word_results, char_results = main()\n",
        "\n",
        "    if word_results is not None and char_results is not None:\n",
        "        print(\"\\nPipeline completed successfully!\")\n",
        "        print(\"You can now analyze the results and CSV files.\")\n",
        "    else:\n",
        "        print(\"\\nPipeline failed. Please check the error messages above.\")\n",
        "        print(\"Make sure to upload your Bangla text file and update the file path.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ubNYlWzFYwlc",
        "outputId": "8f85b109-8292-445a-a4bc-964b7f6fe2b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bangla Text Tokenization Pipeline Starting...\n",
            "REQUIREMENTS:\n",
            "   • Upload a .txt file containing Bangla text\n",
            "   • Update the 'text_file_path' variable with your file path\n",
            "------------------------------------------------------------\n",
            "Looking for text file at: /content/Typed_GroundTruth.txt\n",
            "File loaded successfully with utf-8 encoding\n",
            "Input text length: 1106 characters\n",
            "Input text preview: ৫ আগস্ট অন্তর্বর্তী সরকারের প্রধান উপদেষ্টা অধ্যাপক মুহাম্মদ ইউনূস জাতীয় সংসদের দক্ষিণ প্লাজায় জুলাই...\n",
            "\n",
            "Performing word-level tokenization...\n",
            "Performing character-level tokenization...\n",
            "BANGLA TEXT TOKENIZATION RESULTS\n",
            " \n",
            "\n",
            "SUMMARY STATISTICS\n",
            " \n",
            "Word-level Tokenization:\n",
            "  • Total tokens: 185\n",
            "  • Unique tokens: 138\n",
            "  • Type-token ratio: 0.746\n",
            "\n",
            "Character-level Tokenization:\n",
            "  • Total tokens: 789\n",
            "  • Unique tokens: 167\n",
            "  • Type-token ratio: 0.212\n",
            "\n",
            "TOKEN TYPE DISTRIBUTION\n",
            " \n",
            "Word-level:\n",
            "  • WORD           :  159 ( 85.9%)\n",
            "  • PUNCTUATION    :   20 ( 10.8%)\n",
            "  • OTHER          :    6 (  3.2%)\n",
            "\n",
            "Character-level:\n",
            "  • BANGLA_DIGIT        :    9 (  1.1%)\n",
            "  • WHITESPACE          :  159 ( 20.2%)\n",
            "  • INDEPENDENT_VOWEL   :   46 (  5.8%)\n",
            "  • CONSONANT           :  204 ( 25.9%)\n",
            "  • COMPOUND            :   68 (  8.6%)\n",
            "  • DEPENDENT_VOWEL     :   54 (  6.8%)\n",
            "  • CONSONANT_VOWEL     :  196 ( 24.8%)\n",
            "  • MODIFIER            :   17 (  2.2%)\n",
            "  • OTHER               :   16 (  2.0%)\n",
            "  • PUNCTUATION         :   20 (  2.5%)\n",
            "\n",
            "MOST FREQUENT TOKENS\n",
            " \n",
            "Word-level (Top 10):\n",
            "   1. ',' → 14 times\n",
            "   2. 'করা' → 7 times\n",
            "   3. '।' → 6 times\n",
            "   4. 'ও' → 3 times\n",
            "   5. 'হয়েছে' → 3 times\n",
            "   6. '-' → 3 times\n",
            "   7. 'পরবর্তী' → 3 times\n",
            "   8. 'উল্লেখ' → 3 times\n",
            "   9. '৫' → 2 times\n",
            "  10. 'আগস্ট' → 2 times\n",
            "\n",
            "Character-level (Top 15):\n",
            "   1. ' ' → 151 times\n",
            "   2. 'র' → 41 times\n",
            "   3. 'ক' → 21 times\n",
            "   4. 'ে' → 20 times\n",
            "   5. 'স' → 19 times\n",
            "   6. 'ন' → 18 times\n",
            "   7. 'া' → 17 times\n",
            "   8. 'য' → 17 times\n",
            "   9. '়' → 17 times\n",
            "  10. 'প' → 16 times\n",
            "\n",
            "SAMPLE TOKENS (First 20)\n",
            " \n",
            "Word-level tokens:\n",
            "   1. [WORD        ] '৫'\n",
            "   2. [WORD        ] 'আগস্ট'\n",
            "   3. [WORD        ] 'অন্তর্বর্তী'\n",
            "   4. [WORD        ] 'সরকারের'\n",
            "   5. [WORD        ] 'প্রধান'\n",
            "   6. [WORD        ] 'উপদেষ্টা'\n",
            "   7. [WORD        ] 'অধ্যাপক'\n",
            "   8. [WORD        ] 'মুহাম্মদ'\n",
            "   9. [WORD        ] 'ইউনূস'\n",
            "  10. [WORD        ] 'জাতীয়'\n",
            "  11. [WORD        ] 'সংসদের'\n",
            "  12. [WORD        ] 'দক্ষিণ'\n",
            "  13. [WORD        ] 'প্লাজায়'\n",
            "  14. [WORD        ] 'জুলাই'\n",
            "  15. [WORD        ] 'ঘোষণাপত্র'\n",
            "  16. [WORD        ] 'পাঠ'\n",
            "  17. [WORD        ] 'করলেন'\n",
            "  18. [PUNCTUATION ] '।'\n",
            "  19. [WORD        ] 'এটা'\n",
            "  20. [WORD        ] 'খুব'\n",
            "\n",
            "Character-level tokens:\n",
            "   1. [BANGLA_DIGIT      ] '৫'\n",
            "   2. [INDEPENDENT_VOWEL ] 'আ'\n",
            "   3. [CONSONANT         ] 'গ'\n",
            "   4. [COMPOUND          ] 'স্ট'\n",
            "   5. [INDEPENDENT_VOWEL ] 'অ'\n",
            "   6. [COMPOUND          ] 'ন্ত'\n",
            "   7. [COMPOUND          ] 'র্ব'\n",
            "   8. [COMPOUND          ] 'র্ত'\n",
            "   9. [DEPENDENT_VOWEL   ] 'ী'\n",
            "  10. [CONSONANT         ] 'স'\n",
            "  11. [CONSONANT         ] 'র'\n",
            "  12. [CONSONANT_VOWEL   ] 'কা'\n",
            "  13. [CONSONANT_VOWEL   ] 'রে'\n",
            "  14. [CONSONANT         ] 'র'\n",
            "  15. [COMPOUND          ] 'প্র'\n",
            "  16. [CONSONANT_VOWEL   ] 'ধা'\n",
            "  17. [CONSONANT         ] 'ন'\n",
            "  18. [INDEPENDENT_VOWEL ] 'উ'\n",
            "  19. [CONSONANT         ] 'প'\n",
            "  20. [CONSONANT_VOWEL   ] 'দে'\n",
            "\n",
            "Exporting results...\n",
            "Results exported to CSV files:\n",
            "   • word_tokens.csv\n",
            "   • char_tokens.csv\n",
            "\n",
            "Pipeline completed successfully!\n",
            "You can now analyze the results and CSV files.\n"
          ]
        }
      ]
    }
  ]
}