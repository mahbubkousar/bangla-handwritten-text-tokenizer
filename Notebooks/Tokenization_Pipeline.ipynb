{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Import Libraries"
      ],
      "metadata": {
        "id": "yfJ7IsF7XlDF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import unicodedata\n",
        "from collections import Counter, defaultdict\n",
        "import pandas as pd\n",
        "from typing import List, Dict, Tuple\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "import urllib.request"
      ],
      "metadata": {
        "id": "vRHqLLTyVdte"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Configure Bangla Fonts"
      ],
      "metadata": {
        "id": "-4pQkBRPX2cM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def setup_bangla_fonts():\n",
        "    \"\"\"Download and install Bangla fonts for matplotlib\"\"\"\n",
        "    font_dir = '/usr/share/fonts/truetype/bangla/'\n",
        "    os.makedirs(font_dir, exist_ok=True)\n",
        "\n",
        "    # Download Noto Sans Bengali font\n",
        "    font_url = 'https://github.com/googlefonts/noto-fonts/raw/main/hinted/ttf/NotoSansBengali/NotoSansBengali-Regular.ttf'\n",
        "    font_path = f'{font_dir}NotoSansBengali-Regular.ttf'\n",
        "\n",
        "    if not os.path.exists(font_path):\n",
        "        print(\"Downloading Bangla font...\")\n",
        "        urllib.request.urlretrieve(font_url, font_path)\n",
        "        print(\"Font downloaded successfully!\")\n",
        "\n",
        "        # Clear matplotlib font cache (updated method)\n",
        "        import matplotlib.font_manager as fm\n",
        "        import matplotlib.pyplot as plt\n",
        "\n",
        "        # Delete font cache and rebuild\n",
        "        try:\n",
        "            import shutil\n",
        "            cache_dir = fm.get_cachedir()\n",
        "            if os.path.exists(cache_dir):\n",
        "                shutil.rmtree(cache_dir)\n",
        "            fm.FontManager.__init__(fm.fontManager)\n",
        "            print(\"Font cache rebuilt successfully!\")\n",
        "        except Exception as e:\n",
        "            print(f\"  Font cache rebuild failed: {e}\")\n",
        "            print(\"You may need to restart the runtime for fonts to work properly\")\n",
        "\n",
        "    return 'Noto Sans Bengali'\n",
        "\n",
        "# Setup fonts\n",
        "font_name = setup_bangla_fonts()\n",
        "\n",
        "# Configure matplotlib with fallback fonts\n",
        "plt.rcParams['font.family'] = [font_name, 'DejaVu Sans', 'Liberation Sans', 'sans-serif']\n",
        "plt.rcParams['axes.unicode_minus'] = False  # Handle minus signs properly\n",
        "\n",
        "print(f\"Font configured: {font_name}\")\n",
        "print(\"If Bangla text doesn't display properly, restart runtime and run again\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7-DTXRC5X5fB",
        "outputId": "6c1bdc62-9cfe-4c7b-b98c-f686e6bd0b82"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Font configured: Noto Sans Bengali\n",
            "If Bangla text doesn't display properly, restart runtime and run again\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenizer Class"
      ],
      "metadata": {
        "id": "olUKAZGJYBzG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BanglaTokenizer:\n",
        "    \"\"\"\n",
        "    Bangla Text Tokenizer using Regular Expressions\n",
        "    Implements word-level and character-level tokenization for Bangla text\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        # Word-level tokenization patterns\n",
        "        self.word_patterns = {\n",
        "            'WORD': r'[\\u0980-\\u09FF]+',  # Bangla Unicode block\n",
        "            'NUMBER': r'[\\u09E6-\\u09EF]+',  # Bangla digits\n",
        "            'PUNCTUATION': r'[ред,?!;:]|\\u0964|\\u0965',  # Bangla and borrowed punctuation\n",
        "            'WHITESPACE': r'\\s+',  # Standard whitespace\n",
        "            'LATIN_WORD': r'[a-zA-Z]+',  # Latin characters (for mixed text)\n",
        "            'ARABIC_NUM': r'[0-9]+',  # Arabic numerals\n",
        "            'OTHER': r'[^\\s]'  # Any other character\n",
        "        }\n",
        "\n",
        "        # Character-level tokenization patterns (hierarchical precedence)\n",
        "        self.char_patterns = {\n",
        "            'COMPOUND': r'[\\u0995-\\u09B9](?:\\u09CD[\\u0995-\\u09B9])+',  # Compound consonants\n",
        "            'CONSONANT_VOWEL': r'[\\u0995-\\u09B9][\\u09BE-\\u09CC]',  # Consonant + vowel diacritic\n",
        "            'CONSONANT': r'[\\u0995-\\u09B9]',  # Individual consonants\n",
        "            'INDEPENDENT_VOWEL': r'[\\u0985-\\u0994]',  # Independent vowels\n",
        "            'DEPENDENT_VOWEL': r'[\\u09BE-\\u09CC]',  # Dependent vowel signs\n",
        "            'MODIFIER': r'[\\u09BC\\u09CD\\u09D7]',  # Modifiers and diacritics\n",
        "            'BANGLA_DIGIT': r'[\\u09E6-\\u09EF]',  # Bangla digits\n",
        "            'PUNCTUATION': r'[ред,?!;:]|\\u0964|\\u0965',  # Punctuation\n",
        "            'WHITESPACE': r'\\s',  # Individual whitespace\n",
        "            'LATIN_CHAR': r'[a-zA-Z]',  # Latin characters\n",
        "            'ARABIC_DIGIT': r'[0-9]',  # Arabic numerals\n",
        "            'OTHER': r'.'  # Any other character\n",
        "        }\n",
        "\n",
        "        # Compile patterns for efficiency\n",
        "        self.word_regex = self._compile_patterns(self.word_patterns)\n",
        "        self.char_regex = self._compile_patterns(self.char_patterns)\n",
        "\n",
        "    def _compile_patterns(self, patterns: Dict[str, str]) -> List[Tuple[str, re.Pattern]]:\n",
        "        \"\"\"Compile regex patterns with their labels\"\"\"\n",
        "        return [(label, re.compile(pattern)) for label, pattern in patterns.items()]\n",
        "\n",
        "    def normalize_text(self, text: str) -> str:\n",
        "        \"\"\"Apply Unicode NFC normalization\"\"\"\n",
        "        return unicodedata.normalize('NFC', text)\n",
        "\n",
        "    def word_tokenize(self, text: str) -> List[Dict[str, str]]:\n",
        "        \"\"\"\n",
        "        Perform word-level tokenization\n",
        "        Returns list of dictionaries with token and type\n",
        "        \"\"\"\n",
        "        normalized_text = self.normalize_text(text)\n",
        "        tokens = []\n",
        "        position = 0\n",
        "\n",
        "        while position < len(normalized_text):\n",
        "            matched = False\n",
        "\n",
        "            # Try each pattern in order\n",
        "            for label, pattern in self.word_regex:\n",
        "                match = pattern.match(normalized_text, position)\n",
        "                if match:\n",
        "                    token_text = match.group()\n",
        "                    if label != 'WHITESPACE' or token_text.strip():  # Include whitespace but filter empty\n",
        "                        tokens.append({\n",
        "                            'token': token_text,\n",
        "                            'type': label,\n",
        "                            'position': position,\n",
        "                            'length': len(token_text)\n",
        "                        })\n",
        "                    position = match.end()\n",
        "                    matched = True\n",
        "                    break\n",
        "\n",
        "            if not matched:\n",
        "                # Handle unmatched characters\n",
        "                tokens.append({\n",
        "                    'token': normalized_text[position],\n",
        "                    'type': 'UNKNOWN',\n",
        "                    'position': position,\n",
        "                    'length': 1\n",
        "                })\n",
        "                position += 1\n",
        "\n",
        "        return tokens\n",
        "\n",
        "    def character_tokenize(self, text: str) -> List[Dict[str, str]]:\n",
        "        \"\"\"\n",
        "        Perform character-level tokenization with compound character handling\n",
        "        Returns list of dictionaries with token and type\n",
        "        \"\"\"\n",
        "        normalized_text = self.normalize_text(text)\n",
        "        tokens = []\n",
        "        position = 0\n",
        "\n",
        "        while position < len(normalized_text):\n",
        "            matched = False\n",
        "\n",
        "            # Try patterns in precedence order (compound first)\n",
        "            for label, pattern in self.char_regex:\n",
        "                match = pattern.match(normalized_text, position)\n",
        "                if match:\n",
        "                    token_text = match.group()\n",
        "                    tokens.append({\n",
        "                        'token': token_text,\n",
        "                        'type': label,\n",
        "                        'position': position,\n",
        "                        'length': len(token_text)\n",
        "                    })\n",
        "                    position = match.end()\n",
        "                    matched = True\n",
        "                    break\n",
        "\n",
        "            if not matched:\n",
        "                # Fallback for unmatched characters\n",
        "                tokens.append({\n",
        "                    'token': normalized_text[position],\n",
        "                    'type': 'UNKNOWN',\n",
        "                    'position': position,\n",
        "                    'length': 1\n",
        "                })\n",
        "                position += 1\n",
        "\n",
        "        return tokens\n",
        "\n",
        "    def analyze_tokens(self, tokens: List[Dict[str, str]]) -> Dict:\n",
        "        \"\"\"Analyze tokenization results\"\"\"\n",
        "        token_texts = [t['token'] for t in tokens]\n",
        "        token_types = [t['type'] for t in tokens]\n",
        "\n",
        "        # Count statistics\n",
        "        total_tokens = len(tokens)\n",
        "        unique_tokens = len(set(token_texts))\n",
        "        token_freq = Counter(token_texts)\n",
        "        type_freq = Counter(token_types)\n",
        "\n",
        "        # Most common tokens\n",
        "        most_common_tokens = token_freq.most_common(10)\n",
        "\n",
        "        return {\n",
        "            'total_tokens': total_tokens,\n",
        "            'unique_tokens': unique_tokens,\n",
        "            'token_frequency': token_freq,\n",
        "            'type_frequency': type_freq,\n",
        "            'most_common_tokens': most_common_tokens,\n",
        "            'tokens': tokens\n",
        "        }"
      ],
      "metadata": {
        "id": "7tC4TbHEYDlS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Text File"
      ],
      "metadata": {
        "id": "urRGKB7jYKFJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_text_file(file_path: str) -> str:\n",
        "    \"\"\"Load text from file with proper encoding\"\"\"\n",
        "    if not os.path.exists(file_path):\n",
        "        raise FileNotFoundError(f\"Text file not found at: {file_path}\\n\"\n",
        "                              f\"Please upload a .txt file with Bangla text to: {file_path}\")\n",
        "\n",
        "    # Try different encodings\n",
        "    encodings = ['utf-8', 'utf-16', 'utf-8-sig', 'cp1252']\n",
        "\n",
        "    for encoding in encodings:\n",
        "        try:\n",
        "            with open(file_path, 'r', encoding=encoding) as f:\n",
        "                content = f.read().strip()\n",
        "                if content:  # Check if file is not empty\n",
        "                    print(f\"File loaded successfully with {encoding} encoding\")\n",
        "                    return content\n",
        "                else:\n",
        "                    raise ValueError(\"File is empty\")\n",
        "        except (UnicodeDecodeError, UnicodeError):\n",
        "            continue\n",
        "        except Exception as e:\n",
        "            raise Exception(f\"Error reading file: {str(e)}\")\n",
        "\n",
        "    raise UnicodeDecodeError(\"Could not decode the file with any supported encoding\")\n",
        "\n",
        "def get_sample_text() -> str:\n",
        "    raise NotImplementedError(\"Sample text is not provided. Please upload a .txt file with Bangla text.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "P7doEtE_YM7q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Results Generation"
      ],
      "metadata": {
        "id": "06JrwLDTYkDm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def print_detailed_results(word_analysis: Dict, char_analysis: Dict, sample_size: int = 20):\n",
        "    \"\"\"Print detailed tokenization results\"\"\"\n",
        "    print(\"BANGLA TEXT TOKENIZATION RESULTS\")\n",
        "    print(\" \")\n",
        "\n",
        "    # Summary Statistics\n",
        "    print(\"\\nSUMMARY STATISTICS\")\n",
        "    print(\" \")\n",
        "    print(f\"Word-level Tokenization:\")\n",
        "    print(f\"  тАв Total tokens: {word_analysis['total_tokens']:,}\")\n",
        "    print(f\"  тАв Unique tokens: {word_analysis['unique_tokens']:,}\")\n",
        "    print(f\"  тАв Type-token ratio: {word_analysis['unique_tokens']/word_analysis['total_tokens']:.3f}\")\n",
        "\n",
        "    print(f\"\\nCharacter-level Tokenization:\")\n",
        "    print(f\"  тАв Total tokens: {char_analysis['total_tokens']:,}\")\n",
        "    print(f\"  тАв Unique tokens: {char_analysis['unique_tokens']:,}\")\n",
        "    print(f\"  тАв Type-token ratio: {char_analysis['unique_tokens']/char_analysis['total_tokens']:.3f}\")\n",
        "\n",
        "    # Token Type Distribution\n",
        "    print(\"\\nTOKEN TYPE DISTRIBUTION\")\n",
        "    print(\" \")\n",
        "\n",
        "    print(\"Word-level:\")\n",
        "    for token_type, count in word_analysis['type_frequency'].items():\n",
        "        percentage = (count / word_analysis['total_tokens']) * 100\n",
        "        print(f\"  тАв {token_type:<15}: {count:>4} ({percentage:>5.1f}%)\")\n",
        "\n",
        "    print(\"\\nCharacter-level:\")\n",
        "    for token_type, count in char_analysis['type_frequency'].items():\n",
        "        percentage = (count / char_analysis['total_tokens']) * 100\n",
        "        print(f\"  тАв {token_type:<20}: {count:>4} ({percentage:>5.1f}%)\")\n",
        "\n",
        "    # Most Common Tokens\n",
        "    print(\"\\nMOST FREQUENT TOKENS\")\n",
        "    print(\" \")\n",
        "\n",
        "    print(\"Word-level (Top 10):\")\n",
        "    for i, (token, count) in enumerate(word_analysis['most_common_tokens'][:10], 1):\n",
        "        print(f\"  {i:>2}. '{token}' тЖТ {count} times\")\n",
        "\n",
        "    print(\"\\nCharacter-level (Top 15):\")\n",
        "    for i, (token, count) in enumerate(char_analysis['most_common_tokens'][:15], 1):\n",
        "        print(f\"  {i:>2}. '{token}' тЖТ {count} times\")\n",
        "\n",
        "    # Sample Tokens\n",
        "    print(f\"\\nSAMPLE TOKENS (First {sample_size})\")\n",
        "    print(\" \")\n",
        "\n",
        "    print(\"Word-level tokens:\")\n",
        "    word_tokens = [t for t in word_analysis['tokens'][:sample_size] if t['type'] != 'WHITESPACE'][:sample_size]\n",
        "    for i, token in enumerate(word_tokens, 1):\n",
        "        print(f\"  {i:>2}. [{token['type']:<12}] '{token['token']}'\")\n",
        "\n",
        "    print(f\"\\nCharacter-level tokens:\")\n",
        "    char_tokens = [t for t in char_analysis['tokens'][:sample_size*2] if t['type'] != 'WHITESPACE'][:sample_size]\n",
        "    for i, token in enumerate(char_tokens, 1):\n",
        "        print(f\"  {i:>2}. [{token['type']:<18}] '{token['token']}'\")\n",
        "\n"
      ],
      "metadata": {
        "id": "22r-xJGnYmTr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main Function"
      ],
      "metadata": {
        "id": "C8b7FoLLYp-_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    \"\"\"Main pipeline execution\"\"\"\n",
        "    print(\"Bangla Text Tokenization Pipeline Starting...\")\n",
        "    print(\"REQUIREMENTS:\")\n",
        "    print(\"   тАв Upload a .txt file containing Bangla text\")\n",
        "    print(\"   тАв Update the 'text_file_path' variable with your file path\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "    # Initialize tokenizer\n",
        "    tokenizer = BanglaTokenizer()\n",
        "\n",
        "    # Load text (USER MUST MODIFY THIS PATH)\n",
        "    text_file_path = \"/content/Typed_GroundTruth.txt\"  #\n",
        "\n",
        "    print(f\"Looking for text file at: {text_file_path}\")\n",
        "\n",
        "    try:\n",
        "        text = load_text_file(text_file_path)\n",
        "        print(f\"Input text length: {len(text)} characters\")\n",
        "        print(f\"Input text preview: {text[:100]}...\")\n",
        "\n",
        "        # Perform tokenization\n",
        "        print(\"\\nPerforming word-level tokenization...\")\n",
        "        word_tokens = tokenizer.word_tokenize(text)\n",
        "        word_analysis = tokenizer.analyze_tokens(word_tokens)\n",
        "\n",
        "        print(\"Performing character-level tokenization...\")\n",
        "        char_tokens = tokenizer.character_tokenize(text)\n",
        "        char_analysis = tokenizer.analyze_tokens(char_tokens)\n",
        "\n",
        "        # Display results\n",
        "        print_detailed_results(word_analysis, char_analysis)\n",
        "\n",
        "        # Export results to CSV (optional)\n",
        "        print(\"\\nExporting results...\")\n",
        "\n",
        "        # Word tokens DataFrame\n",
        "        word_df = pd.DataFrame(word_analysis['tokens'])\n",
        "        word_df.to_csv('/content/word_tokens.csv', index=False, encoding='utf-8')\n",
        "\n",
        "        # Character tokens DataFrame\n",
        "        char_df = pd.DataFrame(char_analysis['tokens'])\n",
        "        char_df.to_csv('/content/char_tokens.csv', index=False, encoding='utf-8')\n",
        "\n",
        "        print(\"Results exported to CSV files:\")\n",
        "        print(\"   тАв word_tokens.csv\")\n",
        "        print(\"   тАв char_tokens.csv\")\n",
        "\n",
        "        return word_analysis, char_analysis\n",
        "\n",
        "    except FileNotFoundError as e:\n",
        "        print(f\"ERROR: {e}\")\n",
        "        print(\"\\nTO FIX THIS:\")\n",
        "        print(\"1. Upload your Bangla text file to Colab\")\n",
        "        print(\"2. Update the 'text_file_path' variable in main() function\")\n",
        "        print(\"3. Make sure the file contains Bangla text in UTF-8 encoding\")\n",
        "        return None, None\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR: {e}\")\n",
        "        return None, None\n",
        "\n"
      ],
      "metadata": {
        "id": "6ocftHCYYrq2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Execute the pipeline"
      ],
      "metadata": {
        "id": "MIJnwa9KYtq0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Execute the pipeline\n",
        "if __name__ == \"__main__\":\n",
        "    # Check if results were successful before proceeding\n",
        "    word_results, char_results = main()\n",
        "\n",
        "    if word_results is not None and char_results is not None:\n",
        "        print(\"\\nPipeline completed successfully!\")\n",
        "        print(\"You can now analyze the results and CSV files.\")\n",
        "    else:\n",
        "        print(\"\\nPipeline failed. Please check the error messages above.\")\n",
        "        print(\"Make sure to upload your Bangla text file and update the file path.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ubNYlWzFYwlc",
        "outputId": "8f85b109-8292-445a-a4bc-964b7f6fe2b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bangla Text Tokenization Pipeline Starting...\n",
            "REQUIREMENTS:\n",
            "   тАв Upload a .txt file containing Bangla text\n",
            "   тАв Update the 'text_file_path' variable with your file path\n",
            "------------------------------------------------------------\n",
            "Looking for text file at: /content/Typed_GroundTruth.txt\n",
            "File loaded successfully with utf-8 encoding\n",
            "Input text length: 1106 characters\n",
            "Input text preview: рзл ржЖржЧрж╕рзНржЯ ржЕржирзНрждрж░рзНржмрж░рзНрждрзА рж╕рж░ржХрж╛рж░рзЗрж░ ржкрзНрж░ржзрж╛ржи ржЙржкржжрзЗрж╖рзНржЯрж╛ ржЕржзрзНржпрж╛ржкржХ ржорзБрж╣рж╛ржорзНржоржж ржЗржЙржирзВрж╕ ржЬрж╛рждрзАрзЯ рж╕ржВрж╕ржжрзЗрж░ ржжржХрзНрж╖рж┐ржг ржкрзНрж▓рж╛ржЬрж╛рзЯ ржЬрзБрж▓рж╛ржЗ...\n",
            "\n",
            "Performing word-level tokenization...\n",
            "Performing character-level tokenization...\n",
            "BANGLA TEXT TOKENIZATION RESULTS\n",
            " \n",
            "\n",
            "SUMMARY STATISTICS\n",
            " \n",
            "Word-level Tokenization:\n",
            "  тАв Total tokens: 185\n",
            "  тАв Unique tokens: 138\n",
            "  тАв Type-token ratio: 0.746\n",
            "\n",
            "Character-level Tokenization:\n",
            "  тАв Total tokens: 789\n",
            "  тАв Unique tokens: 167\n",
            "  тАв Type-token ratio: 0.212\n",
            "\n",
            "TOKEN TYPE DISTRIBUTION\n",
            " \n",
            "Word-level:\n",
            "  тАв WORD           :  159 ( 85.9%)\n",
            "  тАв PUNCTUATION    :   20 ( 10.8%)\n",
            "  тАв OTHER          :    6 (  3.2%)\n",
            "\n",
            "Character-level:\n",
            "  тАв BANGLA_DIGIT        :    9 (  1.1%)\n",
            "  тАв WHITESPACE          :  159 ( 20.2%)\n",
            "  тАв INDEPENDENT_VOWEL   :   46 (  5.8%)\n",
            "  тАв CONSONANT           :  204 ( 25.9%)\n",
            "  тАв COMPOUND            :   68 (  8.6%)\n",
            "  тАв DEPENDENT_VOWEL     :   54 (  6.8%)\n",
            "  тАв CONSONANT_VOWEL     :  196 ( 24.8%)\n",
            "  тАв MODIFIER            :   17 (  2.2%)\n",
            "  тАв OTHER               :   16 (  2.0%)\n",
            "  тАв PUNCTUATION         :   20 (  2.5%)\n",
            "\n",
            "MOST FREQUENT TOKENS\n",
            " \n",
            "Word-level (Top 10):\n",
            "   1. ',' тЖТ 14 times\n",
            "   2. 'ржХрж░рж╛' тЖТ 7 times\n",
            "   3. 'ред' тЖТ 6 times\n",
            "   4. 'ржУ' тЖТ 3 times\n",
            "   5. 'рж╣ржпрж╝рзЗржЫрзЗ' тЖТ 3 times\n",
            "   6. '-' тЖТ 3 times\n",
            "   7. 'ржкрж░ржмрж░рзНрждрзА' тЖТ 3 times\n",
            "   8. 'ржЙрж▓рзНрж▓рзЗржЦ' тЖТ 3 times\n",
            "   9. 'рзл' тЖТ 2 times\n",
            "  10. 'ржЖржЧрж╕рзНржЯ' тЖТ 2 times\n",
            "\n",
            "Character-level (Top 15):\n",
            "   1. ' ' тЖТ 151 times\n",
            "   2. 'рж░' тЖТ 41 times\n",
            "   3. 'ржХ' тЖТ 21 times\n",
            "   4. 'рзЗ' тЖТ 20 times\n",
            "   5. 'рж╕' тЖТ 19 times\n",
            "   6. 'ржи' тЖТ 18 times\n",
            "   7. 'рж╛' тЖТ 17 times\n",
            "   8. 'ржп' тЖТ 17 times\n",
            "   9. 'рж╝' тЖТ 17 times\n",
            "  10. 'ржк' тЖТ 16 times\n",
            "\n",
            "SAMPLE TOKENS (First 20)\n",
            " \n",
            "Word-level tokens:\n",
            "   1. [WORD        ] 'рзл'\n",
            "   2. [WORD        ] 'ржЖржЧрж╕рзНржЯ'\n",
            "   3. [WORD        ] 'ржЕржирзНрждрж░рзНржмрж░рзНрждрзА'\n",
            "   4. [WORD        ] 'рж╕рж░ржХрж╛рж░рзЗрж░'\n",
            "   5. [WORD        ] 'ржкрзНрж░ржзрж╛ржи'\n",
            "   6. [WORD        ] 'ржЙржкржжрзЗрж╖рзНржЯрж╛'\n",
            "   7. [WORD        ] 'ржЕржзрзНржпрж╛ржкржХ'\n",
            "   8. [WORD        ] 'ржорзБрж╣рж╛ржорзНржоржж'\n",
            "   9. [WORD        ] 'ржЗржЙржирзВрж╕'\n",
            "  10. [WORD        ] 'ржЬрж╛рждрзАржпрж╝'\n",
            "  11. [WORD        ] 'рж╕ржВрж╕ржжрзЗрж░'\n",
            "  12. [WORD        ] 'ржжржХрзНрж╖рж┐ржг'\n",
            "  13. [WORD        ] 'ржкрзНрж▓рж╛ржЬрж╛ржпрж╝'\n",
            "  14. [WORD        ] 'ржЬрзБрж▓рж╛ржЗ'\n",
            "  15. [WORD        ] 'ржШрзЛрж╖ржгрж╛ржкрждрзНрж░'\n",
            "  16. [WORD        ] 'ржкрж╛ржа'\n",
            "  17. [WORD        ] 'ржХрж░рж▓рзЗржи'\n",
            "  18. [PUNCTUATION ] 'ред'\n",
            "  19. [WORD        ] 'ржПржЯрж╛'\n",
            "  20. [WORD        ] 'ржЦрзБржм'\n",
            "\n",
            "Character-level tokens:\n",
            "   1. [BANGLA_DIGIT      ] 'рзл'\n",
            "   2. [INDEPENDENT_VOWEL ] 'ржЖ'\n",
            "   3. [CONSONANT         ] 'ржЧ'\n",
            "   4. [COMPOUND          ] 'рж╕рзНржЯ'\n",
            "   5. [INDEPENDENT_VOWEL ] 'ржЕ'\n",
            "   6. [COMPOUND          ] 'ржирзНржд'\n",
            "   7. [COMPOUND          ] 'рж░рзНржм'\n",
            "   8. [COMPOUND          ] 'рж░рзНржд'\n",
            "   9. [DEPENDENT_VOWEL   ] 'рзА'\n",
            "  10. [CONSONANT         ] 'рж╕'\n",
            "  11. [CONSONANT         ] 'рж░'\n",
            "  12. [CONSONANT_VOWEL   ] 'ржХрж╛'\n",
            "  13. [CONSONANT_VOWEL   ] 'рж░рзЗ'\n",
            "  14. [CONSONANT         ] 'рж░'\n",
            "  15. [COMPOUND          ] 'ржкрзНрж░'\n",
            "  16. [CONSONANT_VOWEL   ] 'ржзрж╛'\n",
            "  17. [CONSONANT         ] 'ржи'\n",
            "  18. [INDEPENDENT_VOWEL ] 'ржЙ'\n",
            "  19. [CONSONANT         ] 'ржк'\n",
            "  20. [CONSONANT_VOWEL   ] 'ржжрзЗ'\n",
            "\n",
            "Exporting results...\n",
            "Results exported to CSV files:\n",
            "   тАв word_tokens.csv\n",
            "   тАв char_tokens.csv\n",
            "\n",
            "Pipeline completed successfully!\n",
            "You can now analyze the results and CSV files.\n"
          ]
        }
      ]
    }
  ]
}